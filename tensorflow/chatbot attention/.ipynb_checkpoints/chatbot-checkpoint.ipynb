{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# configuration\n",
    "# ------------------------------------------------------------------\n",
    "fromtext_file = 'data/train.from'\n",
    "totext_file = 'data/train.to'\n",
    "\n",
    "length_sentence = 20\n",
    "size_layers = 1028\n",
    "num_layers = 1\n",
    "epoch = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "Train = True\n",
    "\n",
    "location = os.getcwd()\n",
    "checkpoint_directory = location\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dict(fromtext, totext):\n",
    "    \n",
    "    with open(fromtext, 'r') as fopen:\n",
    "        fromtext = fopen.read().split('\\n')\n",
    "    with open(totext, 'r') as fopen:\n",
    "        totext = fopen.read().split('\\n')\n",
    "        \n",
    "    if len(fromtext) != len(totext):\n",
    "        print ('from data-set must has equal length with to data-set, exiting..')\n",
    "        exit(0)\n",
    "\n",
    "    vocab_inputs = []; vocab_predict = []\n",
    "\n",
    "    # we tokenized each sentence in both dataset, turn into vocabulary.\n",
    "#     len(fromtext) -> yang asal\n",
    "#     fromTextSize = len(fromtext)\n",
    "    fromTextSize = 10000 #predefined cause i dont want it take too long\n",
    "    for i in range(fromTextSize):\n",
    "        #insert only unit token in vocab\n",
    "        for keyFrom in [fromtext[i].split()]:\n",
    "            if keyFrom not in vocab_inputs:\n",
    "                vocab_inputs += keyFrom\n",
    "        \n",
    "        for keyTo in [totext[i].split()]:\n",
    "            if keyTo not in vocab_predict:\n",
    "                vocab_predict += keyTo\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "#     print('done tokenizing')\n",
    "    \n",
    "    \n",
    "    # Then we sorted our tokenized words from highest freq to lowest freq.\n",
    "    vocab_inputs = sorted(vocab_inputs, key = vocab_inputs.count,reverse = True)\n",
    "    vocab_predict = sorted(vocab_predict, key = vocab_predict.count,reverse = True)\n",
    "\n",
    "    d1 = dict((k,v) for v,k in enumerate(reversed(vocab_inputs)))\n",
    "    d2 = dict((k,v) for v,k in enumerate(reversed(vocab_predict)))\n",
    "\n",
    "    # Then we turned our sorted words into unique words, while maintaining the position of sorting.\n",
    "    vocab_inputs = ['PAD', 'EOS', 'UNK'] + sorted(d1, key = d1.get, reverse = True)\n",
    "    vocab_predict = ['PAD', 'EOS', 'UNK'] + sorted(d2, key = d2.get, reverse = True)\n",
    "\n",
    "    print ('vocab size for inputs: ' + str(len(vocab_inputs)))\n",
    "    print ('vocab size for predict: ' + str(len(vocab_predict)))\n",
    "\n",
    "    # Then turned into dictionary {'husein': 0, 'suka': 1.. n}\n",
    "    dict_inputs = dict(zip(vocab_inputs, [i for i in range(len(vocab_inputs))]))\n",
    "    dict_predict = dict(zip(vocab_predict, [i for i in range(len(vocab_predict))]))\n",
    "    \n",
    "    import pickle\n",
    "    with open('data/vocab_inputs.p', 'wb') as fopen:\n",
    "        pickle.dump(vocab_inputs, fopen)\n",
    "    with open('data/vocab_predict.p', 'wb') as fopen:\n",
    "        pickle.dump(vocab_predict, fopen)\n",
    "    with open('data/dict_inputs.p', 'wb') as fopen:\n",
    "        pickle.dump(dict_inputs, fopen)\n",
    "    with open('data/dict_predict.p', 'wb') as fopen:\n",
    "        pickle.dump(dict_predict, fopen)\n",
    "    with open('data/fromtext.p', 'wb') as fopen:\n",
    "        pickle.dump(fromtext, fopen)\n",
    "    with open('data/totext.p', 'wb') as fopen:\n",
    "        pickle.dump(totext, fopen)\n",
    "    \n",
    "    return vocab_inputs, vocab_predict, dict_inputs, dict_predict, fromtext, totext , fromTextSize\n",
    "\n",
    "def feed(text, length, dictionary, From = True):\n",
    "    text_int = []\n",
    "    if From:\n",
    "        text_int_decode = [1]\n",
    "    strings = text.split()\n",
    "    for i in range(length):\n",
    "        try:\n",
    "            if From:\n",
    "                text_int.append(dictionary[strings[i]])\n",
    "                text_int_decode.append(dictionary[strings[i]])\n",
    "            else:\n",
    "                text_int.append(dictionary[strings[i]])\n",
    "        #Padding using value of 2 \n",
    "        except KeyError:\n",
    "            text_int.append(2)\n",
    "            if From:\n",
    "                text_int_decode.append(2)               \n",
    "        except IndexError:\n",
    "            text_int.append(0)\n",
    "            if From:\n",
    "                text_int_decode.append(0)\n",
    "                \n",
    "    text_int[length - 1] = 1\n",
    "    \n",
    "    if From:\n",
    "        del text_int_decode[len(text_int_decode) - 1]\n",
    "        return text_int, text_int_decode\n",
    "    else:\n",
    "        return text_int\n",
    "    \n",
    "def graph(LOSS):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "\n",
    "    plt.plot([i for i in range(len(LOSS))], LOSS)\n",
    "    plt.title('loss vs epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def label_to_text(label, vocab):\n",
    "    string = ''\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 0 or label[i] == 1:\n",
    "            continue\n",
    "        string += vocab[label[i]] + ' '\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, num_layers, size_layers, length, learning_rate, vocab_size_input, vocab_size_output):\n",
    "        \n",
    "        self.encoder_inputs = tf.placeholder(shape = [length], dtype = tf.int32)\n",
    "        self.decoder_inputs = tf.placeholder(shape = [length], dtype = tf.int32)\n",
    "        self.decoder_targets = tf.placeholder(shape = [length], dtype = tf.int32)\n",
    "        \n",
    "        def lstm_cell():\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layers, activation = tf.nn.relu)\n",
    "\n",
    "        self.cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        \n",
    "        self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(encoder_inputs = [self.encoder_inputs], \n",
    "                                                                   decoder_inputs = [self.decoder_inputs], \n",
    "                                                                   cell = self.cell, \n",
    "                                                                   num_encoder_symbols = vocab_size_input, \n",
    "                                                                   num_decoder_symbols = vocab_size_input, \n",
    "                                                                   embedding_size = size_layers)        \n",
    "        self.decoder_logits = tf.contrib.layers.linear(self.outputs, len(vocab_predict))\n",
    "        \n",
    "        self.decoder_prediction = tf.argmax(self.decoder_logits, 2)\n",
    "        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = tf.one_hot(self.decoder_targets, depth = vocab_size_output, dtype = tf.float32), \n",
    "                                                                logits = self.decoder_logits)\n",
    "        self.loss = tf.reduce_mean(self.cross_entropy)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load embedded files..\n",
      "unsupported pickle protocol: 3, processing embedded files, this might takes several minutes\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    print (\"load embedded files..\")\n",
    "    fromTextSize = 50000\n",
    "    with open('data/vocab_inputs.p', 'rb') as fopen:\n",
    "        vocab_inputs = pickle.load(fopen)\n",
    "    with open('data/vocab_predict.p', 'rb') as fopen:\n",
    "        vocab_predict = pickle.load(fopen)\n",
    "    with open('data/dict_inputs.p', 'rb') as fopen:\n",
    "        dict_inputs = pickle.load(fopen)\n",
    "    with open('data/dict_predict.p', 'rb') as fopen:\n",
    "        dict_predict = pickle.load(fopen)\n",
    "    with open('data/fromtext.p', 'rb') as fopen:\n",
    "        fromtext = pickle.load(fopen)\n",
    "    with open('data/totext.p', 'rb') as fopen:\n",
    "        totext = pickle.load(fopen)\n",
    "    print ('done load embedded files')\n",
    "except Exception as e:\n",
    "    print (str(e) + ', processing embedded files, this might takes several minutes')\n",
    "    vocab_inputs, vocab_predict, dict_inputs, dict_predict, fromtext, totext, fromTextSize = return_dict(fromtext_file, totext_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(num_layers, size_layers, length_sentence, learning_rate, len(vocab_inputs), len(vocab_predict))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomtest():\n",
    "    import random\n",
    "    randomselect = random.randint(30, 100)\n",
    "    for i in range(randomselect):\n",
    "        num = random.randint(0, fromTextSize - 1)\n",
    "        input_seq_encode, input_seq_decode = feed(fromtext[num], length_sentence, dict_inputs, True)\n",
    "        predict = sess.run(model.decoder_prediction, feed_dict = {model.encoder_inputs : input_seq_encode, model.decoder_inputs : input_seq_decode})\n",
    "        print ('sentence: ' + str(i + 1))\n",
    "        print ('input: ' + fromtext[num])\n",
    "        print ('predict respond: ' + str(label_to_text(predict[0, :], vocab_predict)))\n",
    "        print ('actual respond: ' + totext[num] + '\\n')\n",
    "\n",
    "def test():\n",
    "    sentence = input('> ')\n",
    "    while sentence:\n",
    "        input_seq_encode, input_seq_decode = feed(sentence, length_sentence, dict_inputs, True)\n",
    "        predict = sess.run(model.decoder_prediction, feed_dict = {model.encoder_inputs : input_seq_encode, model.decoder_inputs : input_seq_decode})\n",
    "        print (label_to_text(predict[0, :], vocab_predict))\n",
    "        sentence = input('> ')\n",
    "        \n",
    "def train():   \n",
    "    LOSS = []\n",
    "    for i in range(epoch):\n",
    "        total_loss = 0\n",
    "        lasttime = time.time()\n",
    "        for w in range(fromTextSize):\n",
    "            input_seq_encode, input_seq_decode = feed(fromtext[w], length_sentence, dict_inputs, True)\n",
    "            output_seq = feed(totext[w], length_sentence, dict_predict, False)\n",
    "            _, losses = sess.run([model.optimizer, model.loss], feed_dict = {model.encoder_inputs : input_seq_encode, model.decoder_inputs : input_seq_decode, \n",
    "                                                                 model.decoder_targets : output_seq })\n",
    "            total_loss += losses\n",
    "            \n",
    "            if (w + 1) % 200 == 0:\n",
    "                print ('done process: ' + str(w + 1))\n",
    "                \n",
    "        total_loss = total_loss / (fromTextSize * 1.0)\n",
    "        LOSS.append(total_loss)\n",
    "        print ('epoch: ' + str(i + 1) + (', total loss: ') + str(total_loss) + (', s/epoch: ') + str(time.time() - lasttime))\n",
    "        saver.save(sess, location + \"/model.ckpt\")\n",
    "    graph(LOSS)\n",
    "    randomtest()\n",
    "    \n",
    "def continue_train():\n",
    "    print(\"Load from saved model...\")\n",
    "#     check_current_checkpoint()\n",
    "    saver.restore(sess, location + \"/model.ckpt\")\n",
    "#         checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "#         status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
    "    LOSS = []\n",
    "    for i in range(epoch):\n",
    "        total_loss = 0\n",
    "        lasttime = time.time()\n",
    "        for w in range(fromTextSize):\n",
    "            input_seq_encode, input_seq_decode = feed(fromtext[w], length_sentence, dict_inputs, True)\n",
    "            output_seq = feed(totext[w], length_sentence, dict_predict, False)\n",
    "            _, losses = sess.run([model.optimizer, model.loss], feed_dict = {model.encoder_inputs : input_seq_encode, model.decoder_inputs : input_seq_decode, \n",
    "                                                                 model.decoder_targets : output_seq })\n",
    "            total_loss += losses\n",
    "            \n",
    "            if (w + 1) % 200 == 0:\n",
    "                print ('done process: ' + str(w + 1))\n",
    "                \n",
    "        total_loss = total_loss / (fromTextSize * 1.0)\n",
    "        LOSS.append(total_loss)\n",
    "        print ('epoch: ' + str(i + 1) + (', total loss: ') + str(total_loss) + (', s/epoch: ') + str(time.time() - lasttime))\n",
    "        saver.save(sess, location + \"/model.ckpt\")\n",
    "    graph(LOSS) \n",
    "    \n",
    "def check_current_checkpoint(): \n",
    "    from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "    latest_ckp = tf.train.latest_checkpoint('./')\n",
    "    print_tensors_in_checkpoint_file(latest_ckp, all_tensors=False, tensor_name='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     if Train:\n",
    "    if False:\n",
    "        if tf.train.checkpoint_exists(checkpoint_prefix):\n",
    "            continue_train()\n",
    "        else:\n",
    "            train()\n",
    "    else:\n",
    "        randomtest()\n",
    "        \n",
    "main()\n",
    "\n",
    "# print(checkpoint_directory, \"\\n\", checkpoint_prefix, \"\\n\" ,tf.train.checkpoint_exists(checkpoint_prefix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
